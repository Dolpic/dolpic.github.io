<!---
 TODO :
 - Rename pipeline "steps" into "stages"   
-->

<!DOCTYPE html>
<html>
    <head>
        <link rel="stylesheet" type="text/css" href="../../style.css">
        <title>Occlusion for raster models</title>
        <style>
            body{
                margin-top:50px;
            }
            p{
                text-align: justify;
            }
            img{
                padding:20px;
                max-width: 40%;
            }
            ol, ul{
                margin-left: 30px;
                line-height: 20pt;
                padding: 5px;
            }
            h2{
                text-align: center;
                font-size: 25pt;
                margin-top: 40px;
            }
            h3, summary{
                font-size: 20pt;
                margin: 40px 0px 10px 0px;
                font-weight: bold;
            }
            h4{
                margin: 20px 0px 5px 0px;
            }
            hr{
                width:75%;
                margin:auto;
                margin-top: 45px;
                margin-bottom: 20px;
            }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <h1>DSM Occlusion</h1>
        <p class="subtitle">Occlusion for raster models</p>

        <h3>Software, code & installation</h3>
        <p>Compiled binaries, source code and setup instruction are on the <a href="https://github.com/Dolpic/DSM-Occlusion">project’s GitHub page.</a></p>
        <p>The remaining of this document is dedicated to implementation details.</p>
        
        <hr>
        <h2>Implementation details and theoretical aspects</h2>
        <hr>

        <details>
            <summary>File input & output</summary>
            <p>
                The communication with the file system is handled by the <a href="https://gdal.org/">GDAL library</a> which provides convenient abstractions to read raster in TIFF / GeoTIFF / COG.
                <br>
                During startup, the programs copies the input file into the output file, so that all the output disk space is allocated. 
                The output format is also copied from the input file and handled by the <a href="https://gdal.org/">GDAL library</a>.
                <br>
                File IO operations could be further optimized, but are not a bottleneck. 
                They are mainly linked to the disk read / write speed and the <a href="https://gdal.org/">GDAL</a> implementation.
                Optimizing them further would not lead to a significant gain because of pipelining.
                For very large files however, the first copy operation and the computation of statistics (performed at the very end) may take considerable time. 
            </p>
        </details>
      

        <details>
            <summary>Application pipeline</summary>
            <h4>General overview</h4>
            <p>
                The application splits the input file into tiles, so that the application can handle larger files than would fit in the machine memory.
                <br>
                Processing one tile can be split into 4 steps :
                <ol>
                    <li><b>Tile loading</b> : The time for a tile to be read from the file on disk into memory</li>
                    <li><b>Tile initialization</b> : Before performing ray-tracing on the data, an initialization step is performed on the CPU to create an acceleration structure significantly decreasing the tracing time.</li>
                    <li><b>Tile tracing</b> : The tile is transferred to the GPU where ray-tracing is performed. The result is then transferred back to the CPU.</li>
                    <li><b>Tile writing</b> : Finally, the result is written on the disk</li>
                </ol>
                <i>
                    The steps above are arbitrary, one may choose to split the computation time differently. 
                    For example, two steps can be added before and after the tracing, to take into account the time for the data to be transferred from the CPU to the GPU and from the GPU back to CPU respectively.
                    From experience however, data transfer time between CPU and GPU are mostly small compared to tracing time.
                </i>
            </p>

            <h4>Speedup using pipelining</h4>
            <img style="float:right"  src="ressources/images/piepline.svg">
            <p>
                As tiles are independent of each other (beside border effects discussed below), their computation can be pipelined (inspired from processors <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">instruction pipelining</a>) so that each step defined above does not wait for the whole tile to be finished before processing the next tile.
                This is illustrated by the figure on the right.
                <br><br>
                In the code, the class <i>Pipeline</i> creates one thread per step.
                The main thread ensures synchronization between steps so that each operation starts when the previous is fully completed.
                When a step finishes its work, it notifies the main thread and waits. 
                Upon all steps completion, the main thread moves the data produced by each thread to the input of the next thread and wakes up all the thread.
                <br><br>
                As shown on the figure, the slower step is the bottleneck and dictates when all the steps will switch to the next tile.
                It is therefore crucial to equally spear the workload between threads. 
                The duration of each thread step may vary based on the user parameters (tile size, number of rays, etc.) and also on the hardware used.
                Last generation GPUs significantly decrease the ray-tracing time.
                <br><br>
                During out tests, implementing pipeline improved the whole computation time by about 30%.
                Pipelining could be further improved by splitting the processing time into more steps. 
                However, the bottlenecks are currently the initialization and tracing steps, which, besides data transfer, would be challenging to split.
            </p>

            <h4>Handling tile’s border effects</h4>
            <p>
                The previous section states that tiles are independent of each other. 
                This is not entirely true : near a tile border, geometry located in another tile may influence the shadow.
                By tiling the data, this would lead to geometry being ignored and seeable artifacts in the final result.
                To avoid this issue, the algorithm takes a tile buffer parameter, which serves as overlap between tiles. 
                All tiles are loaded and processed with this overlap, and it is finally ignored when saving the tile.
                <br><br>
                Empirical tests shown that having a buffer of about 1/3 of the tile size prevents artifacts for pixels of 50 cm.
                This parameter is strongly linked to the pixel size, as smaller pixels requires larger overlap.
                <br><br>
                There is a tradeoff when choosing the tile size. 
                On one hand, larger tiles take more time to compute, increasing more than linearly.
                On the other hand, the smaller the tile, the bigger the proportion of buffer and so the time spend computing overlaps later discarded.
                As the optimum depends on the hardware, it can be reached by the user using the <i>tile size</i> and <i>tile buffer</i> parameters.
            </p>
        </details>

        <details>
        <summary>Initialization : constructing a bounding volume hierarchy (BVH)</summary>
        <img style="float:right"  src="ressources/images/BVH.svg">
        <p>
            Starting the path tracing algorithm on raw points would lead to inacceptable execution time, as the process would need to check for every point if any other point occludes it.
            Fortunately, this execution time can be significantly reduced using a bounding volume hierarchy.
            A BVH is a binary tree with leaves describing primitives (part of some geometry) and other nodes describing bounding boxes englobing their children.
            With such a structure, computing the intersection between a ray and all the geometry boils down to a binary tree search. 
            <br>
            During the initialization process, the initial tile's data is converted to a BVH by the CPU. The BVH creation leverages the following simplifications compared to traditional BVHs :
            <ul>
                <li>
                    <b>Only bounded boxes are stored</b> as the data points can be approximated by boxes. 
                    Leaves are bounding boxes around one single data point (one pixel in the original data) and upon intersection with a leaf, the ray is considered to hit the point.
                    Note that in any case an approximation needs to be made : for random rays to hit points in 3D space, these points needs to be made solid. 
                    Approximating points by boxes allow all intersections to be between rays ans boxes, which both simplifies the code and is the fastest intersection to compute.
                </li>
                <li>
                    <b>Nodes store the bounding box of their two children instead of their own. </b>
                    During GPU execution, the bottleneck is the memory accesses to the main memory holding the BVH.
                    To speed up computation, memory accesses are reduced as much as possible.
                    Storing both children bounding boxes at each node prevents loading a whole child node from memory if there is no collision with it.
                </li>
            </ul>

            As all the points are approximated by boxes, there is no need to implement advanced BVH heuristics such as splitting the nodes based on the primitives' surface area.
            Furthermore, as all values in the raster model only gives information about the highest value at the pixel (we have no information if there's a gap under a given pixel), all the bounding boxes have their bottom plane at 0.
            This simplifies some computations.
        </p>
        </details>

        <details>
        <summary>Path tracing</summary>
        <p>
            Once the BVH for a given tile is build, this tile can be processed by the GPU to produce its occlusion.
            For each pixel in the input tile, the GPU launches the user-defined number of rays in randomized directions.
        </p>
        
        <h4>The rendering equation</h4>
        <p>
            To produce the result, the application uses path tracing, a ray-tracing algorithm based on the rendering equation.
            At any given position and time, for every wavelength we have :
            $$L_o(\omega_o)=L_e(\omega_o)+ \int_{\Omega}^{} f_r(\omega_i,\omega_o) \cdot L_i(\omega_i) \cdot (\omega_i \cdot \mathbf{n})\ d\omega_i$$
              
            Where :
            <ul>
                <li>\(\omega_o\) is the direction of the light going away from the surface</li>
                <li>\(\omega_i\) is the direction of the light going toward the surface</li>
                <li>\(L_o(\omega_o)\) is the output radiance (going away from the surface)</li>
                <li>\(L_e(\omega_o)\) is the emitted radiance by surface</li>
                <li>\(\Omega\) is the hemisphere above the flat surface (at infinitesimal scale, every surface appears to be a plane)</li>
                <li>\(f_r(\omega_i,\omega_o)\) is the BRDF, the function that given \(\omega_i\) and \(\omega_o\) outputs the ratio of light reflected</li>
                <li>\(L_i(\omega_i)\) is the incoming radiance (going toward the surface)</li>
                <li>\(\mathbf{n}\) is the normal vector of the surface</li>

            </ul>

            Assuming that the surface do not emit any light and that its material is perfectly diffuse, the expression simplifies to :
            $$L_o(\omega_o)= \int_{\Omega}^{} C \cdot L_i(\omega_i) \cdot cos(\theta_i)\ d\omega_i$$

            Where :
            <ul>
                <li>\(C\) is the BRDF, which is a constant (it does not depend on \(\omega_i\) nor \(\omega_o\) ) for a purely diffuse material.</li>
                <li>\(\theta_i\) is the angle between \(\omega_i\) and \(\mathbf{n}\)</li>
            </ul>
        </p>

        <h4>Monte-Carlo integration</h4>
        <p>
            Evaluating the above integral can only be done analytically for the most simple scenes, which is not the case for our software.
            \(L_i\) depends on the surrounding geometry, and the integral needs to be evaluated each time the light ray bounces.
            <br>
            Such integration can be done using Monte-Carlo integration. 
            In our case the integral can be evaluated by drawing random directions in the hemisphere \(\omega_1, \omega_2, ..., \omega_n\) and computing
            $$L_o(\omega_o) \approx \frac{C}{n}\sum_{j=1}^{n} L_i(\omega_j) \cdot cos(\theta_j) $$
            Where \(\theta_j\) is the angle between \(\omega_j\) and the surface normal.
            <br>
            In the scenario where light does not bounce, \(L_i(\omega_j)\) is a binary function returning 0 if the ray hits part of the geometry and 1 if it escapes toward the sky.
        </p>
        </details>

        <details>
        <summary>Reducing noise</summary>
        <p>
            The Monte-Carlo sampling presented above gives encouraging results, but still requires a tremendous amount of rays (and so computation time) to produce high quality results.
            If not enough rays are traced, the variance of the Monte-Carlo estimator will produce noise in the output image.
            Fortunately, there are techniques to reduce the noise without increases the number of rays.
        </p>

        <h4>Cosine-weighted hemisphere sampling</h4>
        <p>
            Evaluating \(L_o(\omega_o)\) as presented above requires to compute \(cos(\theta_j)\) for each random direction.
            Beside consuming more ressources, this is not the optimal way of evaluating the Monte-Carlo integrator.
            By sampling cosine-weighted random directions, we solve both these issues. 
            The resulting Monte-Carlo integrator has a smaller variance, hence leading to less noise for the same number of rays.
            To randomly sample cosine-weighted directions in the hemisphere, we can use the following formulas :
            
            $$\theta = cos^{-1}(\sqrt{\varepsilon_0})$$
            $$\phi = 2\pi\varepsilon_1$$

            Where
            <ul>
                <li>\(\theta\) is the spherical polar angle</li>
                <li>\(\phi\) is the spherical azimuthal angle</li>
                <li>\(\varepsilon_0\) and \(\varepsilon_1\) are two random variables uniformly distributed in \([0,1]\)</li>
            </ul>
            
            Many websites demonstrate their derivation, like <a href="https://ameye.dev/notes/sampling-the-hemisphere/">this one</a>
        </p>

        <h4>Stratification</h4>
        <p>
            Sampling random directions in the hemisphere may lead to some region being oversampled while other are undersampled.
            This effect increases noise.
            To reduce the noise, a strategy called stratification divided the sample region (the hemisphere in our case) into subareas that will each receive the same number of random-oriented rays.
            The code divides the hemisphere into 16 regions, each of them are divided into subregions along their polar angle, so that each ray falls into one subregion.
        </p>
        </details>

        <details>
        <summary>GPU acceleration</summary>

        <h4>Ray - Box intersection</h4>
        <p>
            The only type of intersection present in the software is the ray - box intersection where the ray is described by a point and a direction, and the box is described by its minimal X,Y,Z and maximal X,Y,Z.
            Thus, as bounding boxes all have their bottom face at height 0, the intersection algorithm simplifies further.

            <b>TODO : Add intersection algorithm</b>
        </p>
        </details>

    </body>
</html>